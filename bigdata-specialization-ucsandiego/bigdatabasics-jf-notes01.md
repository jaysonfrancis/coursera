# Big Data Basics 

### Introduction to Big Data<br>

The Big Data era was kickstarted by the tremendous growth of Cloud Computing and the scale of computing power it made available. This availability of On-Demand Computing coupled with the torrent of Data being generated gives us the opportunity to perform novel, dynamic and scalable data analysis, to tell us new things about our world and ourself.

Through the use of Big Data companies are able to create better marketing campaigns and reach the right customers. Example:

 - Reccomendtation Enginges : Leverage usage pattern to give better suggestions to the users.
 - Sentiment Analysis : Analyze the general opinion / public opinion of a given entity using Natural Language Processing.
 - Mobile Advertising : Use realtime sensor-data and Geolocation Data to serve targeted ads. 
 - Collective Consumer Behaviours : Businesses can understand their consumerâ€™s collective behavior and hence target the right audience for their product.
 - BioMedical Applications : Enabling personalized medicine through analysis of genomics.
 - IOT / Sensor Data : Analysis of integrated sensor data from various sources for better prediction and management.

#### Where does big data come from?

Big Data boils down to 3 major sources of data:

1. **Machines**

   - Data generated by Sensors / Smart Devices often known as the *Internet of Things* or **IoT**
   - Largest and fastest of all the Big Data sources
   - Quite complex in structure
   - Enables real-time processes to be put in place or *in-situ* analytical processing.
   - Culture shift needed for its computing and real-time action as scalable computing systems are required.

2. **People**

   - Data generated by the activity of people/users on various platforms(Example:Social Media, Blogs,etc..)
   - Tend to be heavily text/image based and unstructured in nature
   - Challenging to deal with.
   - High volume and fast generation of data
   - Expensive to deal with and process 

3. **Organizations** 

   - Data generated by various organizations(Example: Banks, Governement, Medical Records, etc..)

   - Considered the traditional form of data which is typically used by most organizations today.

   - Data tends to be highly structured in nature, usually stored in Relational Database Management Systems or Spreadsheets

   - Data generation practices vary from organization to organization.

   - Easy to query

   - May lead to data being *siloed* ( Data being compartmentalized in various departments of the organizations)

   - Siloed data leads to outdated, unsynchronized or even invisible data and poor integration between data sources.

     

#### Integrating Diverse Data sources

Data integration means bringing together data from diverse sources and 

turning them into coherent and more useful information. This may also be referred to as **knowledge**.

A data integration process involves many parts. It starts with **discovering**, **accessing**, and **monitoring** data and continues with **modeling** and **transforming** data from a variety of sources.

The data then becomes more available for use and unified as a system of its own.

### Characteristics of Big Data<br>

####The Big Vs of Big Data<br>

- **Volume**: Refers to the vast amounts of data that is generated every second, minute, hour and day. 
    - Digital data will grow by a factor of 44, to around 35.2 ZB. 
    - There are some challenges in massive volume, which include storage, data acquisition, retrieval, distribution and processing. 

- **Velocity**: _(Speed)_ Refers to the speed at which data is being generated and the pace at which data moves from one point to the next.
    - `Velocity = Speed - (Change of X / Change of T)`
    - Batch Processing: Collect Data, Clean, Feed in Chunks, Wait, Act.
    - Realtime Processing: Instantly capture streaming data, feed realtime, process real-time, act.

- **Variety**: _(Complexity)_ Increasing forms that data can come in such as text, images, voice and geospatial data, etc. 
    - Can be structured in several dimensions
        - **Structural Variety:** Difference in the representation of the data. Formats and models. (e.g, an EKG signal is very different from a news paper article)
        - **Media Variety:** Medium in which data gets delivered. (e.g, Audio speech vs transcripts)
        - **Semantic Variety:** How to interpret and operate on data. (e.g, age can be a number or terms like infant, juvenile, adult, etc.)
        - **Availability Variations**: Time intervals and frequency at which the data is made available(Continuous traffic data vs Intermittent Satellite data)

> Some additional _Vs_ to remember

- **Veracity:** _(Quality)_ Refers to the biases, noise and abnormality in the data.
    - Data has no value if it's not accurate. ( Junk in = Junk out )
    - **Data Providence:** What information did data go through up until the moment it was used for an estimate?
    - The growing torrents of big data pushes it for fast solutions. This creates challenges of data quality, what has been collected, where it came from and how it was analyzed.

- **Valence:** _(Connectedness)_ Connectedness of big data in the form of graphs.
    - The more connected the data is, the higher the valence. 
    - Valence challenges include more complex data exploration algorithms, modeling and prediction of valence changes, gorup event detections, emergent behavior analysis.

- **Value:** How does big data benefit you and your organization. Turning data into opportunities.

---
### Data Science, Getting Value out of Big Data

Data science can be thought of as a basis for empirical research where data is used to induce information for observations.
Data scientist are people who have passion for data, can relate problems to analytics, care about engineering solutions to solve problems, exhibit curiosity, and communicate well. 

#### Skills related to Data Science
- **Math and Statistics**
    - Machine Learning, Statistical modeling, experiment design, Bayesian inference
    - Supervised learning: Decision trees, random forest, logistic regression
    - Unsupervised learning: Clustering, dimensionality reduction.
    - Optimization: Gradient descent and variants.
- **Programming & Databases**
    - Computer science fundamentals, scripting languages (e.g python), statistical computing packages (e.g, R), databases (e.g, SQL, MongoDB)
    - Relational algebra
    - Parallel processing, distributed computing
    - Hadoop, Hive, Pig, MapReduce concepts
- **Domain Knowledge & Soft Skills**
    - Passionate about the business, curious about the data
    - Influence without authority. 
    - Hacker mindset, problem solving.
    - Strategic, proactive, creative, innovative & collaborative.
- **Communication & Visualization**
    - Able to engage with senior management
    - Storytelling skills, translate data-driven insights into decisions and actions.
    - Visual art design, D3.js, Tableau, Flare
    
#### Five Components of Data Science
- **People:** Refers to the data science team. (Engineers, Business)
- **Purpose:** Refers to the challenges defined by your strategy. 
- **Process:** Define a process to collaborite and communicate around. 
    - Big Data Engineering, Computation Big Data Science (Advanced Analytics)
    - Acquire Data --> Prepare Data --> Analyze Data --> Generate Reports --> Act
- **Platforms:** Hadoop Framework, Scalability, Infrastructure, etc.
- **Progammability:** Should be programmable through reusable interfaces.



### Process of Data Analysis

1. **Accquire Data**

   - Traditional DBs
   - Text Files
   - Web Services and REST APIs

   

2. **Prepare Data**

   <ol type="A">
     <li>Explore
       <ul style="list-style-type:circle;">
         <li>Corellations</li>
         <li>General Trends</li>
       </ul>
     </li>
     <li>Pre Process(Clean, Integrate and Package)
       <ul style="list-style-type:circle;">
       	<li>Address Data Quality Issues(Missing Values, Inconsistencies, etc..)</li>
         <li>Data Wrangling(Scaling, Transformation, Feature Selection, Dimensionality Reduction, etc..)</li>
       </ul>
       </li>
   </ol>
   

3. **Analyze Data**(Select Technique and Build Models)

   Model Building: 

   - Classification(Goal:Predict Category)
   - Clustering(Goal: Organize similar datapoints into groups)
   - Regression(Goal: Predict value)
   - Graph Analysis(Goal: Find connections between entities in a graph)
   - Association Analysis(Goal: Find rules to capture association between items)

4. **Report and Communicate Results(**Evaluation of results and presenting them)

5. **Act** (Report actionable insights from the analysis)



### Distributed File Systems

Distributed File Systems(DFS) are File Systems where data sets, or parts of a data set, can be replicated/spread across the nodes and then analysis of parts of the data can be done in a data-parallel fashion by moving computation to these nodes rather than the other way round as in traditional File Systems. This allows the system to be easily scaled in response to more data.

However, the drawback with this type of File System is that consistency may be an issue between replicated data sets.



### Scalable Computing over the Internet

#### Parallel Computer

- Large number of Single Computer nodes 
- Expensive
- Very specialized for the task

#### Commodity Cluster

- Computing nodes clustered in racks, connected via a *fast* network
- Each node is cheaper and less specialized
- Several points of failure
- High potential for node level failures, hence needs to achieve fault tolerance:
  - Redundant Data Storage
  - Data Parallel Job Restart
- Uses **MapReduce** as the Programming Model

